### ì„¹ì…˜ 1

**Hadoop ê¸°ë³¸ ì„¸íŒ…**
- Virtualbox
- HDP (Hortonworks Sandbox 2.6.5)

**Hadoop ê°œìš” ë° ì—­ì‚¬**
- ë¶„ì‚° ì €ì¥/ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼
- ë³‘ë ¬ êµ¬ì¡°ë¥¼ í†µí•´ í•œê°œì˜ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ì»´í“¨í„° (í´ëŸ¬ìŠ¤í„°) ë¡œ ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬ ê°€ëŠ¥
	- í´ëŸ¬ìŠ¤í„°ëŠ” ëŒ€ì—¬ ê°€ëŠ¥í•œ ì¼ë°˜ì ì¸ ì»´í“¨í„°ë¡œ êµ¬ì„±

- GFS (Google File System)
	- Hadoop file system ì˜ ì¡°ìƒ
	- ë¶„ì‚° ì €ì¥ì†Œì˜ ì²« ë„ì…

- í•˜ë‘¡ ì œì‘ì : Yahoo

- Default : Horizontal scaling
- Batch processing ì„ ìœ„í•´ ì²˜ìŒ ë„ì…


### Hadoop Architecture ê°œìš”

1. **Core Hadoop Ecosystem**
	- **HDFS (Hadoop Distributed File System)**
		- distribute storage of big data across cluster
		- maintains redundant copies of data
			- can recover from incidents (backup)
	- **YARN (Yet another resource negotiator)**
		- manages the resources on your computing cluster
			- descides what gets to run when
			- what nodes are available, which are not, ...
		- 
		- **Map Reduce**
			- programming metaphor/model that allows you to process data across an entire cluster
			- consist of mappers and reducers
			- Mappers
				- ability to transform data in parallel across your entire computing cluster
			- Reducers
				- aggregate that data together
			- 
			- **pig**
				- high level programming api that allows you to write simple scripts that look like sql
				- transform script into what will run on map reduce
					- will go through yarn and hdfs to process and get the data that is needed
			- **Hive**
				- looks like a sql database
				- can execute sql queries that is stored on your hadoop cluster
		- **Spark**
			- run queries on data
			- map reduce it
			- handle real time streaming data and etc
		- **Tez**
			- uses a directed acyclic graph and uses same techniques as spark
			- produce more optimal plans for actually executing queries
	- **Apache Ambari**
		- gives the view of your cluster and let you visualize the states of your cluster
	- **Mesos**
		- alternate for yarn
	- **HBase**
		- no-sql database
		- fast db for large transaction rates
		- fast way of exposing results to other systems
	- **Storm**
		- way of processing streaming data
		- storm/spark solves the same problem
		- does not have to be a batch
		- can update ml models or transform data into database
	- **oozie**
		- schedule all of these things together into jobs that can be run on some schedule
	- **zookeeper**
		- coordinating everything on your cluster
		- keep track of shared states across cluster
		- can be used for keeping track of who the master node is or who's up who's down
	
	**Data Ingestion**
	- **Scoop**
		- connector between rdbms to hadoop database
	- **Flume**
		- transport web logs to your cluster (real time)
			- processed by storm or spark streaming
	- **kafka**
		- collect data of any sort from a cluster of pc or web servers
		- broadcaset that into your hadoop cluster

2. **External Data Storage**
	- **MySQL**
		- rdbms, etc
	- **Cassandra**
	- **MongoDB**
		- columnar data stores
		- for exposing data in real time usage
		- used as layer between real time application and cluster
	- **Hbase**

3. **Query Engines**
	- **Drill**
		- write sql queries that will work across range of no-sql databases
		- so that they can talk to hbase, cassandra, mongodb
		- and tie those results together and allow you to write queries across them
	- **Hue**
		- interactively creating queries that works well with hive and hbase
	- **pheonix**
		- do sql style queries
		- guarantee acid and oltp

### Section 2

**HDFS**
- what allows data to be stored across cluster in a distributed and reliable manner
- for handling large files
- break them up into blocks
- store more than one copy of each block (duplicates) for backup upon incidents
- allow you to purchase "commodity" computers

**Name Node**
- keep track of where all the blocks live
- what file partition is in which cluster

**Data Node**
- what actually store each files


**Fetch data**
Client Node -A, B-> Name Node -cluster 1, cluster 2-> Client Node -> Data Node

**Write data**
Client Node -ima write a, b, c-> Name Node - okay -> Client node -a, b, c -> Data Node - make copy -> Data Node2 -make copy-> Data Node 3 -i wrote it-> Client Node -> Name Node


**why is there only name node?**
- there should be only one name node active at a time or else clients might not agree on where different blocks are stored


**Backup metadata**
- name node writes to local disk and nfs mount
- if name node dies, it can bootstrap it using logs


**Secondary Name Node**
- maintains merged copy of edit log from primary name node


**HDFS Federation**
- when there are lots of small files, name node will reach its breaking point
- namespace volume (sub directories)
- given volume will know which name to talk to for reading/writing data files
- we can have separate name structure for each volume
	- given volume will know which name node to talk to, reading and writing data


**HDFS high availability**
- use shared edit log (shared storage that is not hdfs)
- the backup can take over right away
- use zookeeper to track active name node
	- does have more complications


**Map Reduce**
- divide up all data into partitions that can be processed in parallel across cluster
- **Maps Data**
	- transform data
	- extract information that it cares about, and organize it so it makes sense
- **Reduce Data**
	- aggregates data together
	- associate data with certain key value (something you want to aggregate on)
- **handle failure**
- natively java

**Example**
Bob : Antz, Forest Gump, Dumbo
Jessica : Lion King, Nemo, Toy Story

INPUT DATA --> Mapper --> (K1:V, K2:V, K3:V, K1:V, K1:V) (key/value pair)
                         (Bob : Antz, Bob : Forest Gump, Bob : Dumbo, Jessica : Nemo, ...)


*Goal : How many movies did each person rate?*

1. Mapper transfer input data into individual movie rating and transfer it into key-value pair
	- Key : user_id (what to aggregate on)
	- Value : movies (what will be aggretating)
- **same key can appear multiple times and its desireable**


```csv
USER ID|MOVIE ID|RATING|TIMESTAMP
196 242 3 88123123
186 302 3 12314122
196 277 1 12312344
244 51  2 45632413
166 346 1 43562345
...
```

**--> mapper** 
- mapping stage may be distributed across different computers

--> 196:242, 186:302, 196:377, 244:51, ...
- it is important to drop everything that is not needed in the earlier stage of the process so excess data does not get moved around the clusters
	- meaning : optimization is important

**--> SHUFFLE AND SORT (Automatic)** 
- sorts and groups the mapped data

--> 166:346  186:302,274,265  196:242,377  244:51


**Reducer**
- given output from shuffle and sort, reducer is a bit of code that you provide that descides what to do with it

--> **len(movies)**

--> 166:1  186:3  196:2  244:1


---

**ë” ìƒì„¸í•œ ê³¼ì •**

client - i want a mr job -> yarn
client - copy data "up" that is needed for mr job -> hdfs

mapreduce application master starts on Node Manager
Node manager : keeps track of what this node is doing, is it up, etc,,,

application master
- responsible for keeping an eye on all the mr task
- works with the resource manager to distribute that work across clusters

yarn (resource manager)
- will try to make sure that the mr job run as close to the data

**Streaming**
- map task can just kick off through std i/o talking to other process that is happening that actually implement mappers and reducers

**Failure**
1. Application master monitors worker tasks for errors or hanging
	1. Restarts as needed
	2. Preferably on a different node
2. What if the application master goes down
	1. Yarn can try to restart it
3. What if an entire node goes down
	1. yarn can try to restart it
4. what if yarn (resource manager) goes down
	1. can set up "high availability" using zookeeper to have a hot standby resource manager


**Making a problem into a MapReduce Problem**
Goal : count the  number of each rating

```python
from mrjob.job import MRJob
from mrjob.step import MRStep

class RatingsBreakdown(MRJob):
	def steps(self):
		return [
			MRStep(mapper=self.mapper_get_ratings,
				reducer=self.reducer_count_ratings)
		]

	# mapper
	def mapper_get_ratings(self, _, line):
		(user_id, movie_id, rating, timestamp) = line.split('\t')
		yield rating, i

	# shuffle & sort happens in MRStep automatically without specifying

	# reducer
	def reducer_count_rating(self, key, values):
		yield key, sum(values)

if __name__ == '__main__':
	RatingsBreakdown.run()

```

**SSH hdp**
`ssh maria_dev@127.0.0.1 -p 2222`

**Multi-stage jobs**
```python
return [
	MRStep(mapper=self.mapper_get_ratings,
		reducer=self.reducer_count_ratings),
	MRStep(reducer=self.reducer_sorted_output)
]
```

---
### Section 3

**Ambari**
- `127.0.0.1:8080`
- overview of what your cluester is doing
- can start off with ambari to set up the rest with specific services you want

*how to open ambari under "Admin" instead of "maria dev"*
- su admin
- ambari-admin-password-reset

**Pig (Pig Latin)**
- create map reduce jobs without having to write Mappers and Reducers
- scripting language lets you use sql like syntax to define your map and reduce steps
- highly extensible with user defined functions

hdfs -> yarn -> mapreduce / tez -> pig

- can translate its jobs into mr or tez

**Grunt**
- command line (execute one at a time)

**Script**
**Ambari / Hue**


**pig script example**
- find the oldest 5 star rating movies
```pig
-- find the oldest 5 star rating movies

-- creates a "relation" named ratings with a given schema
ratings = LOAD '/user/maria_dev/m1-100k/u.data' AS
	(userId:int, movieId:int, rating:int, ratingTime:int)

-- using pigstorage if you need a different delimeter
metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')
	AS (movieID:int, movieTitle:chararray, releaseDate:chararray,
		videoRelease:chararray, imdbLink:chararray);

-- Creating relation from another relation
metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')
	AS (movieID:int, movieTitle:chararray, releaseDate:chararray,
		videoRelease:chararray, imdbLink:chararray);

/*
                      (second time since 1970)
	01-Jan-1995      -->      788918400
*/

nameLookup = FOREACH metadata GENERATE movie ID, movieTitle,
		ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) AS releaseTime;


-- Group By
ratingsByMovie = GROUP ratings BY movieID;


-- average Ratings
avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID,
	AVG(ratings.rating) AS avgRating;

-- Filter

fiveStarMovies = FILTER avgRatings BY avgRating > 4.0;

-- JOIN

fiveStarsWithData = JOIN fiveStarMovies BY movieID, nameLookup BY movieID;

-- ORDER BY
oldestFiveStarMovies = ORDER fiveStarsWithData BY nameLookup::releaseTime;


```

***To run this even faster***
- does not have to use map reduce, but rather use TEZ
	- click button



**Pig Latin Commands**
- LOAD, STORE, DUMP
	- STORE ratings INTO 'outRatings' USING PigStorage(':');
		- write them in a file with delimeter :
- FILTER, DISTINCT, FOREACH/GENERATE, MAPREDUCE, STREAM, SAMPLE
- JOIN, COGROUP, GROUP, CROSS, CUBE
	- cogroup : creates seperate tuple for each key and create nested structure
	- group : reduce
	- cross : cartesian product
	- cube : all combination for more than two columns
- ORDER, RANK, LIMIT
	- rank : assign rank number
	- limit : head(5)
- UNION, SPLIT
	- union : put them into one relation

**Diagnostics**
- DESCRIBE
- EXPLAIN
- ILLUSTRARE

**UDF**
- REGISTER
- DEFINE
- IMPORT


**Other functions**
- AVG, CONCAT, COUNT, MAX, MIN, SIZE, SUM

- pigstorage
	- field based data (set delimiter)
- textloader
- jsonloader
- avrostorage
- parquetloader
- orcstorage
- hbasestorage

**pig challenge**

*Goal 1 : Find all movies with and average rating less than 2.0*
*Goal 2 : Sort them by the total number of ratings*


```pig
-- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° 1
ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS (userID:int, movieID:int, rating:int, ratingTime:int);

-- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° 2
metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')
	AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRealese:chararray, imdblink:chararray);

-- for looking up movie title based on movie id (create subset)
nameLookup = FOREACH metadata GENERATE movieID, movieTitle;

-- mapping (group ratings by movieid)
groupedRatings = GROUP ratings BY movieID;

-- for each grouped movie, create columns 
-- 1. average of the ratings as avgRating
-- 2. count of the ratings as numRatings
averageRatings = FOREACH groupedRatings GENERATE group AS movieID,
	AVG(ratings.rating) AS avgRating, COUNT(ratings.rating) AS numRatings;

-- create filter
badMovies = Filter averageRatings BY avgRating < 2.0;
```


---

### Section 4


**Spark**
- fast and general engine for large-scale data processing
- its scalable
	- driver program (spark context) -> cluster manager (spark, yarn) -> executor (cache, tasks)

- its fast
	- 100x faster than mapreduce in memory, 10x faster on disk
- DAG Engine (directed acyclic graph) optimizes workflows
- written in scala


- code in Python, Java, Scala
- built around : Resilient Distributed Dataset (RDD)
	- object that represent dataset, and various functions you can call on that RDD to 
		- transform
		- reduce
		- analyze

**Components of Spark**

 Spark Streaming | Spark SQL | MLLib | GraphX |
------------------------------------------------
 |                              Spark Core                              |

**Spark Streaming**
- instead of batch processing, takes in live streaming data (log)
- analyzed across window of time
- can store it

**Spark SQL**
- SQL interface for spark
- can use sql like functions to transform dataset in spark

 **MLLib**
 - library of ml and tools
 - clustering or regression analysis into mapper and reducers

**GraphX**
- analyze property of graph


 *ultimately, running things on scala is most optimal vs python*


**RDD (Resilient Distributed Dataset)**
- make sure your job is evenly distributed across clusters
- can handle failures in resilient manner

**RDD Objects**
- storing key - value information in an object that can automatically do the right thing on a cluster
- resilient and distributed

**basically like a spreadsheet broken into chunks, spread across multiple computers where you can run operations like filter, map and reduce in parallel**


**Spark Context**
- environment that you are running RDDs in, which are created by driver program
- it creates RDD's
- spark shell creates a "sc" object for you


***Crate RDD's***
```
# direct init example (not practical)
nums = parallelize([1,2,3,4])

# load file from hdfs, s3, ...
sc.textFile("file:///c:/users/frank/gobs-o-text.txt")
	- or s3n:// , hdfs://

# using hive
hiveCtx = HiveContext(sc) rows = hiveCtx.sql("select name, age from users")

also available from
- JDBC
- Cassandra
- Hbase
- Elasticsearch
- JSON, CSV, sequence files, object files, various compressed formats
```


**Transforming RDD's**
- map
	- 1:1 relationship
	- one row outputs to another row
```python
rdd = sc.parallelize([1, 2, 3])
mapped = rdd.map(lambda x: x * 2)  # [2, 4, 6]
```
- flatmap
	- N:M relationship
	- input lines may result in one or more
```python
rdd = sc.parallelize(["hello world", "spark is fast"])
flat = rdd.flatMap(lambda line: line.split())  
# [["hello", "world"], ["spark", "is", "fast"]]
# ["hello", "world", "spark", "is", "fast"]
```
- filter
	- filter can take stuff out of RDD
```python
rdd = sc.parallelize([1, 2, 3, 4])
filtered = rdd.filter(lambda x: x % 2 == 0)  # [2, 4]
```
- distinct
	- give you distinct unique value within RDD
- sample
- union, intersection, subtract, cartesian

- groupByKey
```python
rdd = sc.parallelize([
    ("movie1", 3), ("movie1", 4), ("movie2", 2)
])
grouped = rdd.groupByKey()
# [("movie1", [3, 4]), ("movie2", [2])]
```
- mapValues
```python
rdd = sc.parallelize([
    ("movie1", (10, 2))
])
mapped = rdd.mapValues(lambda x: x[0] / x[1])  # [("movie1", 5.0)]
```
- join(otherRDD)
```python
rdd1 = sc.parallelize([("movie1", "Action")])
rdd2 = sc.parallelize([("movie1", 5.0)])
joined = rdd1.join(rdd2)
# [("movie1", ("Action", 5.0))]
```

**Actions (reduce/compute)**
- collect ()
	- take whatever in rdd to -> python list
- count
	- count number of elements
- countByValue
	- how many does each unique value occurs
- take
	- returns the first n elements
- top
- reduce
	- define function that will combine all values associated with each unique key
- reduceByKey
```python
rdd = sc.parallelize([
    ("movie1", 3), ("movie1", 4), ("movie2", 2)
])
reduced = rdd.reduceByKey(lambda a, b: a + b)
# [("movie1", 7), ("movie2", 2)]
```



**Lazy evaluation**
- nothing actually happens in your driver program until an **action** is called
- then, spark will say, i will work backwards from that result and figure out the fastest way to achieve that result
- until you hit an action, all you are doing is building chain of dependencies

**RDD to fine a lowest avg rating movies**

```python
from pyspark import SparkConf, SparkContext

# create dictionary so we can convert movieID to movie Name for the final result
def loadMovieNames(): # {1:'aaa', 2:'bbb', 3:'ccc'}
	movieNames = {}
	with open('ml-100k/u.item') as f:
		for line in f:
			fields = line.split('|')
			movieNames[int(fields[0])] = fields[1]
	return movieNames


# take each line of u.data and convert it to (movieID, (rating, 1.0))
# add up all the ratings for each movie, and the total number of ratings from each movie

def parseInput(line): # (movieID, (rating, 1.0))
	fields = line.split()
	return (int(fields[1]), (float(fields[2]), 1.0))


if __name__ == "__main__":
	conf = SparkConf().setAppName("WorstMovies")
	sc = SparkContext(conf=conf)

	# ì¶”í›„ ì´ë¦„ ë³€ê²½ì„ ìœ„í•´ ë§Œë“  dict
	movieNames = loadMovieNames()

	# read data from hdfs storage
	lines = sc.textFile("hdfs:///user/maria_dev/ml-100k/u.data") # lines : RDD

	# convert data into structure (movidID, (rating, 1.0))
	movieRatings = lines.map(parseInput) # movieRating : RDD

	# reduce (movieID, (sum, count))
	ratingTotalsAndCount = movieRatings.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))

	# map to averageRating
	averageRatings = ratingTotalsAndCount.mapValues(lambda totalandcount: totalandcount[0] / totalandcount[1])

	# sort by average rating
	sortedMovies = averageRatings.sortBy(lambda x: x[1])

	# take top 10 results
	results = sortedMovies.take(10)

	for result in results:
		print(movieNames[result[0]], result[1])

```
- strategy
	- for each movieID
		- sum up all the rating
		- sum up how many rating exist
		- calculate average using the sums

- ***when you do a reduce operation on RDD, you get passed in two values***
	- for each unique key, you are going to be passed pair of values that will be combined together in some way


**SC._____**
- `sc.parallelize`
	- splits it across spark workers so it can be processed in parallel
`rdd = sc.parallelize([1, 2, 3, 4, 5], numSlices=2)`

|Method|Purpose|
|---|---|
|`sc.textFile(path)`|Reads a text file (HDFS, S3, local, etc.) into an RDD of lines|
|`sc.wholeTextFiles(path)`|Reads files as (filename, content) pairs|
|`sc.sequenceFile(path)`|Reads Hadoop SequenceFiles|
|`sc.parallelize(data)`|Converts Python collection to RDD|
|`sc.broadcast(value)`|Broadcasts a variable to all worker nodes|
|`sc.accumulator(value)`|Shared counter (accumulator) used across workers|
|`sc.setCheckpointDir(path)`|Sets a directory for RDD checkpointing|
|`sc.addFile(path)`|Distributes a file to all executors (can be local or remote)|

***When running spark python scripts, you have to use spark-submit shell***
`spark-submit` shell
- sets up spark environment for you
- make sure its running on cluster
	- can pass in parameters to specify which cluster you want to run on
	- how much memory you want to allocate

`spark-submit LowestRatedMovieSpark.py`


**Spark SQL**
- Extends RDD to "DataFrame" object
	- contain row objects
	- can run sql queries
	- has a schema
		- can store data more efficiently, transport and optimize things based on schema information
	- read and write to json, hive parquet
	- communicates with JDBC / ODBC, Tableau

**SparkSQL in Python**
- `from pyspark.sql import SQLContext, Row`
- **Hive**
	- `hiveContext = HiveContext(sc)`
- **JSON -> DF**
	- `inputData = spark.read.json(dataFile)`
**create table view from dataframe**
	- `inputData.createOrReplaceTempView("myStructuredStuff")`
**running sql query on hive context**
- `myResultDataFrame = hiveContext.sql("""SELECT foo FROM bar ORDER BY foobar""")`

**df commands**
- df.show()
- df.select("field_name")
- df.filter(df("field_name" > 200))
- df.groupBy(df(field_name)).mean()
- df.rdd().map(mapperFunction)

**Datasets**
- Dataframe = DataSet of Row objects

#### [replace]
##### 1. ğŸ”¥ Use Modern Spark (ignore `SQLContext`, `HiveContext` unless absolutely needed)

`from pyspark.sql import SparkSession  spark = SparkSession.builder.appName("AppName").getOrCreate()`

âœ… This single `spark` object replaces all of:

- `SQLContext`
    
- `HiveContext`
    
- Legacy confusion
    

---

##### 2. ğŸ“¥ Read JSON â†’ DataFrame

python

ë³µì‚¬í¸ì§‘

`df = spark.read.json("data.json")`

- JSON becomes a **DataFrame** (think table with columns and types).
    
- You can `.show()`, `.filter()`, `.groupBy()`, etc.
    

---

##### 3. ğŸ·ï¸ Create a Temp View from DataFrame (optional, for SQL)

python

ë³µì‚¬í¸ì§‘

`df.createOrReplaceTempView("people") result = spark.sql("SELECT name FROM people WHERE age > 25")`

- Now you're using **SQL** on a DataFrame by referencing it as a table called `"people"`.
    

---

##### 4. ğŸ§ª DataFrame API vs SQL

You can do the same logic two ways:

|SQL|DataFrame|
|---|---|
|`SELECT name FROM people WHERE age>25`|`df.filter(df.age > 25).select("name")`|

They both return a new **DataFrame**.

---

##### 5. ğŸ” `.rdd()` (only if you _must_ drop back to RDDs)

python

ë³µì‚¬í¸ì§‘

`rdd = df.rdd.map(lambda row: row.name)`

âœ… Don't use `.rdd()` unless:

- You're doing something low-level that DataFrame API doesn't support
    
- You _already_ understand RDDs and have a reason to drop into them

[/replace]














**Lowest Rating movie**
```python
from pyspark import SparkConf, SparkContext

# This function just creates a Python "dictionary" we can later
# use to convert movie ID's to movie names while printing out
# the final results.
def loadMovieNames():
    movieNames = {}
    with open("ml-100k/u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

# Take each line of u.data and convert it to (movieID, (rating, 1.0))
# This way we can then add up all the ratings for each movie, and
# the total number of ratings for each movie (which lets us compute the average)
def parseInput(line):
    fields = line.split()
    return (int(fields[1]), (float(fields[2]), 1.0))

if __name__ == "__main__":
    # The main script - create our SparkContext
    conf = SparkConf().setAppName("WorstMovies")
    sc = SparkContext(conf = conf)

    # Load up our movie ID -> movie name lookup table
    movieNames = loadMovieNames()

    # Load up the raw u.data file
    lines = sc.textFile("hdfs:///user/maria_dev/ml-100k/u.data")

    # Convert to (movieID, (rating, 1.0))
    movieRatings = lines.map(parseInput)

    # Reduce to (movieID, (sumOfRatings, totalRatings))
    ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: ( movie1[0] + movie2[0], movie1[1] + movie2[1] ) )

    # Filter out movies rated 10 or fewer times
    popularTotalsAndCount = ratingTotalsAndCount.filter(lambda x: x[1][1] > 10)

    # Map to (rating, averageRating)
    averageRatings = popularTotalsAndCount.mapValues(lambda totalAndCount : totalAndCount[0] / totalAndCount[1])
    
    # Sort by average rating
    sortedMovies = averageRatings.sortBy(lambda x: x[1])

    # Take the top 10 results
    results = sortedMovies.take(10)

    # Print them out:
    for result in results:
        print(movieNames[result[0]], result[1])
```


**MLLIB**
- built in machine learning library for spark

**ALS**
- recommendation algorithm
- `als = ALS(maxIter=5, regParam=0.01, userCol="userID, itemCol="movieID", ratingCol="rating")`

**Cache DataFrame**
- `ratings = spark.createDataFrame(ratingsRDD).cache()`

**filter movies that have at least ten rating**
- **RDD version**
	- `popularTotalsAndCount = ratingTotalsAndCount.filter(lambda x: x[1][1] > 10`
- **DataFrame version**
```python
movieDataSet = spark.createDataFrame(movies)

# movieID, averageRating
averageRatings = movieDataSet.groupBy("movieID").avg("rating")

# movieID, count
counts = movieDataSet.groupBy("movieID").count()

joined = counts.join(averageRatings, "movieID").filter("count > 10")
```

---

### Section 5

**Hive**
- HDFS í´ëŸ¬ìŠ¤í„° ì „ì²´ì— ê±¸ì³ ì €ì¥ëœ ë°ì´í„°ì— í‘œì¤€ SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰
- SQL -> MR or TEZ
- YARN í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ì ìœ„ì—ì„œ ì‹¤í–‰
- give the experience of using a sql data warehouse
- **HiveQL**
	- variant of SQL
- lot easier to just use hive and sql syntax than write map reduce in java
- for OLAP, not fit for OLTP
	- for analytic queries across large dataset
- everything is denormalized

**extensible**
- user defined functions
- thrift server
- jdbc / odbc driver

**When to not use hive**
- OLTP
	- it converts sql commands into MR and MR takes time to spin up

**Views**
- can store the results of one query into a view and then use that view as a table in subsequent queries
	- way of breaking up more complicated queries into individual queries
	- it will persist, but not store a copy of that data anywhere

**Schema On Read**
- it takes an unstructured data, and applies a schema to it as it is being read instead
- **HCatalog**
	- expose that schema to other services as well

```sql
create table ratings (
	userID int,
	movieID int,
	rating int,
	time int
)
row format delimited
fields terminated by 't'
stored as textfile;

load data local inpath '${env:HOME}/ml-100k/u.data'
overwrite into table ratings;
```

**Load Data**
- move data from a distributed file system into hive

**Load Data Local**
- copies data from your local filesystem into hive

**Managed vs External Tables**
- Managed
	- load data
		- copy of data that is owned by hive
		- if you drop it, its gone
- External
	- when you create it, and give explicit location
	- if you drop it, its going to leave it there
```sql
create external table if not exists ratings(
	userID int,
	movieID int,
	rating int,
	time int
)
row format delimited fields terminated by '\t'
location '/data/ml-100k/u.data'
```

**Partitioning (optimization)**
- if you have massive dataset and if your queries are tend to be a specific partition of that data

```sql
create table customers (
	name string,
	address struct<street:string, city:string, state:string, zip:int>
)
partitioned by (country string);
```
- does treat country like a column
- it will become part of a sub directory under where hive stores its data
	- .../customers/country=CA/
	- .../customers/country=GB/

 **How to access hive**
 1. CLI / prompt
 2. saved query files
	 - `hive -f /somepath/queries.hql`
3. through ambari / hue
4. through jdbc / odbc server
	1. **jdbc**
		- Java Database Connectivity
		- a way for java applications to talk to a database
	2. **odbc**
		- general API for any application to connect to database using drivers
5. through thrift service
6. via oozie

**integrating MySQL to Hadoop**

**MySQL**
- generally monolithic in nature
- can be use for oltp

**Scoop**
- handle large dataset
- import export them to cluster
- RDBMS -> Mapper -> HDFS

`scoop import --conect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies --hive-import`

`-m` : specify how many mapper you use

**incremental imports**
- keep RDBMS and Hadoop
- `--check-column
	- timestamp or sequence number
- --last-value`
	- where

**Sqoop : Export data from Hive to MySQL**
`sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001'`

**ì‹¤ìŠµ**
1. import movielens data into mysql database
```sql
set names 'utf8';
set character set utf8;
use movielens;
source movielens.sql
```
2. import movies to HDFS
```linux
sqoop import --connect jdbc:mysql://localhost/movielens --username root --password hadoop --table movies -m 1 2> error.log
```
3. import movies into Hive
```linux
sqoop import --connect jdbc:mysql://localhost/movielens --username root --password hadoop --table movies -m 1 --hive-import 2> error.log
```
4. export movies back into MySQL
apps > hive > warehouse > movies
```linux
sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --username root --password hadoop --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001' 2> error.log
```



### Section 6

**Hbase**
- opensource implementation of google's big table
- only has CRUD (Create, Read, Update, Delete) operation API
	- no query language

- **HMaster**
	- what manage schema of the data
	- knows where everything is
- **Region Server**
	- data are stored on hdfs
- **Zookeeper**
	- who watches the watcher
	- track who the current master is

- êµ¬ì„±
	- **Row**
		- referenced by a unique KEY
		- each ROW has some small number of COLUMN FAMILIES
		- **COLUMN FAMILIES**
			- contain arbitrary COLUMNS
	- **CELL**
		- can have many VERSIONS with given timestamps

- êµ¬ì„± Example
	- Key
		- com.cnn.www
	- **Column Family**
		- **Contents**
			- `<html><head>CNN...</html></html>`
	- **Anchor column family**
	- store all pages that link back go cnn.com
		- **Anchor:cnnsi.com**
			- "CNN"
		- **Anchor:my.look.ca**
			- "CNN.com"

----
### MongoDB ê°œìš”
- **MongoDB**ëŠ” ê¸°ì—… ì§€ì›ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëŒ€í‘œì ì¸ NoSQL DBë¡œ, ì´ë¦„ì€ â€œHuMONGOus dataâ€ì—ì„œ ìœ ë˜.    
- íŠ¹ì§•ì€ **ë¬¸ì„œ ê¸°ë°˜(Document) ë°ì´í„° ëª¨ë¸**ë¡œ, JSON í˜•ì‹ ë°ì´í„°ë¥¼ ììœ ë¡­ê²Œ ì €ì¥ ê°€ëŠ¥. ìŠ¤í‚¤ë§ˆ ê°•ì œê°€ ì—†ìœ¼ë©°, í•„ìš”í•˜ë©´ ìŠ¤í‚¤ë§ˆë¥¼ ì ìš©í•  ìˆ˜ë„ ìˆìŒ.
    

### CAP ì •ë¦¬ì—ì„œì˜ ìœ„ì¹˜
- **ì¼ê´€ì„±(Consistency) + íŒŒí‹°ì…˜ í—ˆìš©(Partition Tolerance)** ì„ íƒ.
- ë‹¨ì¼ **Primary ë…¸ë“œ**ì— ì“°ê¸°ê°€ ì§‘ì¤‘ â†’ Primary ì¥ì•  ì‹œ, ìƒˆë¡œìš´ Primary ì„ ì¶œê¹Œì§€ ì“°ê¸° ë¶ˆê°€(ì¼ì‹œì  ê°€ìš©ì„± í•˜ë½).
- ì½ê¸°ëŠ” Secondaryì—ì„œ ê°€ëŠ¥í•˜ì§€ë§Œ ê¶Œì¥ë˜ì§€ ì•ŠìŒ.
    

### ê¸°ë³¸ êµ¬ì¡°
- **ë°ì´í„°ë² ì´ìŠ¤ â†’ ì»¬ë ‰ì…˜(Collection) â†’ ë¬¸ì„œ(Document)** êµ¬ì¡°.
- ê° ë¬¸ì„œì— `_id` ê¸°ë³¸ í‚¤ ìë™ ìƒì„±.
- ì¸ë±ìŠ¤ë¥¼ ë‹¤ì–‘í•œ í•„ë“œÂ·ì¡°í•©ì— ìƒì„± ê°€ëŠ¥.
- ì¡°ì¸(Join)ì´ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ **ë¹„ì •ê·œí™”(De-normalization)** ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ê³„ í•„ìš”.
    

### ë³µì œ(Replica Set)
- Primaryâ€“Secondary êµ¬ì¡°ë¡œ ë°ì´í„° ë³µì œ.
- Primary ì¥ì•  ì‹œ Secondaryê°€ ì„ ì¶œë˜ì–´ ëŒ€ì²´.
- **ê³¼ë°˜ìˆ˜ ì„ ì¶œ** í•„ìš” â†’ ìµœì†Œ 3ëŒ€ ì„œë²„ í•„ìš”.
- ë¹„ìš© ì ˆê°ì„ ìœ„í•´ **Arbiter ë…¸ë“œ** ì‚¬ìš© ê°€ëŠ¥(íˆ¬í‘œë§Œ ë‹´ë‹¹).
- **ì§€ì—° ë³µì œ(Delayed Secondary)** ì„¤ì • ê°€ëŠ¥ â†’ ì˜ëª»ëœ ì‘ì—…(ì˜ˆ: drop DB) ë³µêµ¬ìš©.
    

### ìƒ¤ë”©(Sharding) â€“ ë¹…ë°ì´í„° í™•ì¥
- ì—¬ëŸ¬ Replica Setì„ **ìƒ¤ë“œ í‚¤(Shard Key)** ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë°ì´í„° ë¶„ì‚° ì €ì¥.
- **mongos í”„ë¡œì„¸ìŠ¤**ì™€ **Config ì„œë²„**ê°€ ë¼ìš°íŒ… ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬.
- **Balancer**ê°€ ë°ì´í„° ë¶„í¬ ë¶ˆê· í˜• ì‹œ ìë™ ì¬ì¡°ì •.
- ìƒ¤ë“œ í‚¤ëŠ” **ê³ ìœ ì„±(High Cardinality)** ë†’ì€ í•„ë“œê°€ ì í•©.
    

### ì¥ì  ë° ì¶”ê°€ ê¸°ëŠ¥
- **ìœ ì—°ì„±**: JSON í˜•íƒœì˜ ë¹„ì •í˜• ë°ì´í„° ììœ ë¡­ê²Œ ì €ì¥.
- **ê°•ë ¥í•œ ì¸ë±ìŠ¤**: í…ìŠ¤íŠ¸, ì§€ë¦¬ ê³µê°„(Geospatial) ì¸ë±ìŠ¤ ì§€ì›.
- **Aggregation** ë° **MapReduce** ì§€ì› â†’ ì¼ë¶€ Hadoop ê¸°ëŠ¥ ëŒ€ì²´ ê°€ëŠ¥.
- **GridFS** ì œê³µ â†’ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì €ì¥ ê°€ëŠ¥.
- **SQL ì»¤ë„¥í„°** ì œê³µ â†’ SQL ì¿¼ë¦¬ ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ RDBì²˜ëŸ¼ íš¨ìœ¨ì  ì¡°ì¸ì€ ë¶ˆê°€.
- **Spark/Hadoop ì—°ê³„** ê°€ëŠ¥, ì¼ë¶€ ì—°ì‚°ì„ MongoDB ë‚´ë¶€ë¡œ í‘¸ì‹œë‹¤ìš´í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ.

---

## ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹œ ê³ ë ¤ ì‚¬í•­

### 1. ì‹œìŠ¤í…œ í†µí•©ì„±
- ì–´ë–¤ **ë‹¤ë¥¸ ê¸°ìˆ ê³¼ ì—°ê²°**í•´ì•¼ í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸.
- Spark, SQL ì¸í„°í˜ì´ìŠ¤ ë“±ê³¼ì˜ í˜¸í™˜ì„±ì´ ì„ íƒì§€ë¥¼ ì œí•œí•  ìˆ˜ ìˆìŒ.
### 2. í™•ì¥ì„± (Scale)
- ë°ì´í„° í¬ê¸°: ë‹¨ì¼ ì„œë²„ í•œê³„ ì´ˆê³¼ ì˜ˆìƒ â†’ **Cassandra, MongoDB, HBase** ê°™ì€ ë¶„ì‚° DB í•„ìš”.
- íŠ¸ëœì­ì…˜ ì²˜ë¦¬ëŸ‰: ì´ˆë‹¹ ìˆ˜ì²œ ê±´ ì´ìƒ ìš”ì²­ â†’ ë¶„ì‚° êµ¬ì¡° í•„ìš”.
### 3. ìš´ì˜ ë° ë³´ì•ˆ
- ë‚´ë¶€ì— ì „ë¬¸ê°€ê°€ ìˆëŠ”ì§€ í™•ì¸.
- NoSQLì€ ê¸°ë³¸ê°’ìœ¼ë¡œëŠ” ë³´ì•ˆì´ ì·¨ì•½ â†’ ì˜¬ë°”ë¥¸ ì„¤ì • í•„ìš”.
- ì „ë¬¸ê°€ ë¶€ì¬ ì‹œ **ìœ ë£Œ ì§€ì›(ì˜ˆ: MongoDB ê¸°ì—… ì§€ì›)** ê³ ë ¤.
### 4. ì˜ˆì‚°
- ëŒ€ë¶€ë¶„ ì˜¤í”ˆì†ŒìŠ¤ë¼ **ì†Œí”„íŠ¸ì›¨ì–´ ë¹„ìš©ì€ ë¬´ë£Œ**.
- ì£¼ìš” ë¹„ìš© = **ì„œë²„/í´ë¼ìš°ë“œ ë¹„ìš© + ìš´ì˜ ì¸ë ¥ ë¹„ìš©**.
- AWS, GCP ê°™ì€ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ í™œìš© ê°€ëŠ¥.
### 5. CAP ì •ë¦¬
- **Partition Tolerance**ëŠ” ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì—ì„  í•„ìˆ˜.
- **Consistency vs Availability** ì¤‘ ë¬´ì—‡ì„ ìš°ì„ í• ì§€ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ê²©ì— ë”°ë¼ ê²°ì •.
    - ê¸ˆìœµ/ì£¼ì‹ê±°ë˜ â†’ **Consistency** ì¤‘ì‹œ.
    - ì›¹ ì„œë¹„ìŠ¤/ì¶”ì²œ ì‹œìŠ¤í…œ â†’ **Availability** ì¤‘ì‹œ.
### 6. ë‹¨ìˆœì„±
- í•„ìš” ì´ìƒìœ¼ë¡œ ë³µì¡í•œ NoSQL ì•„í‚¤í…ì²˜ ë„ì… ê¸ˆì§€.
- ê·œëª¨ê°€ ì‘ìœ¼ë©´ **MySQL ê°™ì€ ë‹¨ìˆœ RDB**ê°€ ë” í•©ë¦¬ì .
    

---

## ì‚¬ë¡€ë³„ ì ‘ê·¼

1. **ë‚´ë¶€ ì „í™”ë²ˆí˜¸ë¶€ (ì†Œê·œëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜)**
    - ë°ì´í„° ì ìŒ, íŠ¸ëœì­ì…˜ ì ìŒ â†’ **MySQL** ì í•©.
    - ë‹¨ìˆœ, ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš°ê°€ ë§ìŒ.
        
2. **ì›¹ ë¡œê·¸ ë¶„ì„ (ë‚´ë¶€ ë¶„ì„ìš©)**
    - ëŒ€ê·œëª¨ íŠ¸ëœì­ì…˜ ì—†ìŒ â†’ DB ë¶ˆí•„ìš”.
    - **Hadoop, Spark, Hive, Pig** í™œìš©.
        
3. **ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ (ëŒ€ê·œëª¨ ì‚¬ìš©ì, ë¹ ë¥¸ ì‘ë‹µ í•„ìš”)**
    - **Availability + Partition Tolerance** ìš°ì„ .
    - **Cassandra** ì í•©. (ConsistencyëŠ” ì¡°ê¸ˆ í¬ìƒ ê°€ëŠ¥)
        
4. **ì£¼ì‹ ê±°ë˜ ì‹œìŠ¤í…œ (ëŒ€ê·œëª¨ íŠ¸ëœì­ì…˜, ê°•í•œ ì¼ê´€ì„± í•„ìš”)**
    - **Consistency + Partition Tolerance** í•„ìˆ˜.
    - ë³´ì•ˆ/ì§€ì› ì¤‘ìš” â†’ **MongoDB, HBase, í˜¹ì€ ê³ ê¸‰ RDB ë¶„ì‚° êµ¬ì¡°** ê³ ë ¤.
    - ìœ ë£Œ ì§€ì›ê³¼ ê°•ë ¥í•œ ë³´ì•ˆ ì²´ê³„ í•„ìš”.
---

ğŸ‘‰ í•µì‹¬:  
ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì€ **í†µí•©ì„±, í™•ì¥ì„±, ìš´ì˜/ë³´ì•ˆ, CAP ìš°ì„ ìˆœìœ„, ë‹¨ìˆœì„±**ì„ ê¸°ì¤€ìœ¼ë¡œ ê²°ì •.  
ì‘ì€ ê·œëª¨ = **RDB (MySQL)**,  
ëŒ€ê·œëª¨ ë¶„ì„ = **Hadoop/Spark**,  
ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ = **NoSQL (Cassandra/MongoDB/HBase)**,  
ê¸ˆìœµ/ê±°ë˜ = **Consistency ì¤‘ì‹œ DB**.


---

**2025-09-02**

**Yarn (yet another resource negotiator)**
- ë¦¬ì†ŒìŠ¤ êµì„­ìë¥¼ ë¶„ë¦¬
- cluster compute layer (hdfs ì €ì¥ì†Œ ë ˆì´ì–´ ìœ„ì—ì„œ ì‘ë™)
- ìœ„ì— spark/tez ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰
	- ì§ì ‘ì ìœ¼ë¡œ yarn ì— ì½”ë”© í•„ìš” ì—†ìŒ
- yarn : **í´ëŸ¬ìŠ¤í„°ì˜ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**
- hdfs : **í´ëŸ¬ìŠ¤í„°ì˜ ì €ì¥ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**

**ì‘ë™ ë°©ì‹**
- MR script (client node) -> YARN -> Node ê´€ë¦¬ì -> MR ì• í”Œë¦¬ì¼€ì´ì…˜ ë§ˆìŠ¤í„° -> YARN -> HDFS cluster - data -> YARN
- yarn ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì„¸ìŠ¤ (mr) ì„ ì–´ë””ì—ì„œ ì‹¤í–‰í• ì§€ ì•Œì•„ë‚´ëŠ” ê²ƒ
	- ë…¸ë“œ êµ¬ë¶„
	- ë¶„ì‚° ë°©ë²•
	- ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ë°ì´í„° ì´ë™ ìµœì†Œí™”
	- í´ëŸ¬ìŠ¤í„°ì˜ CPU ì‚¬ìš© ìµœì í™”
	- ë°ì´í„°ë¥¼ ì§€ì—­ì ìœ¼ë¡œ ìœ ì§€

application <-> yarn
- distribute workload to cluster
- -> zookeeper -> track master node

**ì‹¤íŒ¨ ë³µì›ë ¥**
- Schedule Option
	- FIFO
		- 1st application -> 2nd application
	- Capacity
		- ì—¬ìœ  ìˆëŠ” ì‰ì—¬ ìš©ëŸ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
	- Fair
		- ëŒ€ê·œëª¨ ì²˜ë¦¬ ìš©ëŸ‰ ì¼ë¶€ë¥¼ ê°€ì ¸ì™€ì„œ ì‘ì€ ì‘ì—…ì— í• ë‹¹

**Tez**
- í•˜ì´ë¸Œ, í”¼ê·¸, MR ì˜ ì‘ì—…ì„ ê°€ì†
	- ê¸°ì¡´ ì‘ì—… ë°©ì‹ì„ ê´€ì°° í›„ í•„ìš” ì—†ëŠ” ë‹¨ê³„ ì œê±°ì™€ ê°™ì´ ë” íš¨ìœ¨ì ì¸ ë°©ë²• ê²€ìƒ‰
- MRì˜ ëŒ€ì•ˆ
- ë¯¸ë¦¬ ì„¤ì¹˜ëœ ê¸°ë³¸ ê°’ìœ¼ë¡œ ì„¤ì •/ì¡°ì‘í•  ê²ƒì´ ë”±íˆ ì—†ìŒ


**Mesos**
- distributed data management system
- fetch all hardware pool and distribute work load
- kinda like yarn but not only for hadoop
- not technically part of hadoop but some hadoop component may communicate with mesos instead of yarn

- **Myriad**
	- package that merge yarn with mesos
	- via myriad, yarn can communicate with mesos and use its resource

**mesos vs yarn**
- yarn : single scheduler
	- for big data and hadoop cluster (optimized for longer runtime)
- mesos : two tiered system
	- mesos offers resources to framework
	- for huge computing resource and multiple variant of task with different lifecycle

- í•˜ë‘¡ ë¿ë§Œì´ ì•„ë‹ˆë¼ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì•„í‚¤í…ì²˜ì— ì í•©
	- ë¶„ì‚° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ëŒ€ëŸ‰ ì†Œìœ  ë° ì‹¤í–‰
	- best if
		- you are using Spark & Storm & analyze big data


**Zookeeper**
- ì „ì²´ í´ëŸ¬ìŠ¤í„°ì— ë°ì´í„°ë¥¼ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•˜ëŠ” ì‹œìŠ¤í…œ
- í´ëŸ¬ìŠ¤í„°ì˜ ë¶€ë¶„ ì‹¤íŒ¨ë¥¼ íšŒë³µí•˜ëŠ”ë° ì‚¬ìš©
	- ì‹¤íŒ¨
		- ê°œë³„ ë…¸ë“œ ë‹¤ìš´
		- ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
		- ë„¤íŠ¸ì›Œí¬ê°€ ì´ìƒí•˜ê²Œ íŒŒí‹°ì…˜
		- í•˜ë“œ ë“œë¼ì´ë¸Œ ì‹¤íŒ¨
- ë¶„ì‚° ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€
- Specific
	- ë§ˆìŠ¤í„° ì„ ì¶œ (ë§ˆìŠ¤í„°ë¥¼ ì¶”ì í•˜ê³  ê·¸ ë§ˆìŠ¤í„°ê°€ ë‹¤ìš´ë˜ë©´ ìƒíƒœ íŒŒì•…)
		- ë‹¤ë¥¸ ë°±ì—… ë§ˆìŠ¤í„° í•˜ë‚˜ë¥¼ ì„ ë³„í•´ ìƒˆ ë§ˆìŠ¤í„°ë¡œ ì§€ì • í›„ ì¶”ì 
	- í¬ë˜ì‹œ íƒì§€(ì‘ì—…ì ë‹¤ìš´)
		- ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì´ ì‚¬ì‹¤ì„ ì „ë‹¬í•˜ê³  ì‘ì—…ì„ ë‹¤ì‹œ ë¶„ì‚°
	- ê·¸ë£¹ ê´€ë¦¬
		- ì–´ë–¤ ì‘ì—…ìê°€ ê°€ìš© ê°€ëŠ¥í•œì§€ ì¶”ì 
	- í´ëŸ¬ìŠ¤í„°ì™€ ê´€ë ¨ëœ ë©”íƒ€ë°ì´í„° ìœ ì§€
		- ë¯¸ê²° ì‘ì—… (outstanding tasks)
		- ì‘ì—… ë¶€ì—¬ (task assignment)
	- znode (master)
		- í˜„ì¬ ë§ˆìŠ¤í„°ì— ëŒ€í•œ ë°ì´í„° ë³´ìœ 
	- ```
	  /
	  |
	  - - /master   "master1.foobar.com:2223"
	  |
	  - - /workers
		    |
		     - - worker-1 "worker-2.foobar.com:2225"
		    |
		     - - worker-2 "worker-5.foobar.com:2225"
	  ```
		- master1.foobar.com ì´ ë‹¤ìš´ë˜ë©´ í•´ë‹¹ ë…¸ë“œë¥¼ ë†“ì•„ì£¼ê³  ë‹¤ë¥¸ ë…¸ë“œê°€ ê·¸ ìë¦¬ë¥¼ ì°¨ì§€
			- zookeeper ì€ í•œë²ˆì— í•˜ë‚˜ì˜ í´ë¼ì´ì–¸íŠ¸ë§Œ ë§ˆìŠ¤í„° znodeì˜ ì£¼ê¶Œì„ ê°–ë„ë¡ ì¡°ì •
		- ì”ë¥˜ì„±
			- ì‘ì—…ìì—ê²Œ ë¶€ì—¬í•œ ì‘ì—…ì€ ë§ˆìŠ¤í„° ë…¸ë“œê°€ ì‹¤íŒ¨í•´ë„ ì”ë¥˜í•´ì•¼ í•¨
	- zookeeper ensemble
		- ë‘ê°œ ì´ìƒì˜ ì£¼í‚¤í¼ ì„œë²„ê°€ í•„ìš”
			- ì•„ë‹ˆë©´ ë‹¨ì¼ ì‹¤í•´ ì§€ì ì´ ë¨
		- zookeeper client must have all server's list
	- **ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ìœ„í•´ 5ê°œì˜ ì„œë²„ì™€ ì •ì¡±ìˆ˜ 3ì´ í•„ìš”**


**OOZIE**
- í•˜ë‘¡ í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ê¸°ì ì¸ ì‘ì—…ì„ ê´€ë¦¬ (ìŠ¤ì¼€ì¤„ ë° ì‹¤í–‰)
	- ì—¬ëŸ¬ ì¡°ê°ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì•¡ì…˜ì„ ì •ë¦¬, í•œ ë° ë¬¶ì–´ì„œ ì›Œí¬í”Œë¡œë¥¼ ì œì‘
	- ì›Œí¬í”Œë¡œê°€ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë„ë¡ ìŠ¤ì¼€ì¤„
- ì›Œí¬í”Œë¡œ
	- í•˜ë‘¡ì˜ ì„œë¡œ **ì˜ì¡´**í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì„ í•œë° ë¬¶ìŒ
		- ë‹¨ì¼ ì•¡ì…˜
		- ì—°ê²°ëœ ì•¡ì…˜ (MR > HIVE > Pig > Sqoop > distcp)
		- ì‘ì—…ê°„ ì¢…ì†ì„± êµ¬ì¡° ì„¤ì • ê°€ëŠ¥
		- ë¬¶ì¸ ì•¡ì…˜ì— DAG ì„¤ì • ê°€ëŠ¥
- Fork Node
	- ë‘ê°œ ì´ìƒì˜ action ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ë•Œ
- join Node
	- fork ë…¸ë“œì˜ ë³‘ë ¬ action ì´ ë§Œë‚˜ëŠ” ì§€ì 
	- ë‘ ì¢…ì†ì„±ì´ ì™„ë£Œë˜ê¸°ë¥¼ ëŒ€ê¸°
- êµ¬ì„± ë°©ë²•
	- ê°œë³„ ì•¡ì…˜ì´ ìŠ¤ìŠ¤ë¡œ ì˜ ì‘ë™í•´ì•¼ í•¨
		- ì‚¬ì „ì— ê°œë°œ í›„ í…ŒìŠ¤íŠ¸
			- oozie ì—ì„œ ë””ë²„ê¹…ì€ ì‰½ì§€ ì•ŠìŒ
	- êµ¬ì„± íŒŒì¼ì„ hdfs ì— ì €ì¥ (xml)
	- job.properties íŒŒì¼ ìƒì„±
		- workflow.xml íŒŒì¼ì—ì„œ ì°¸ì¡°í•  ë³€ìˆ˜ ì •ì˜
- ì½”ë””ë„¤ì´í„°
	- ì‹œì‘ ì‹œê°„ê³¼ ì‹¤í–‰ ì£¼ê¸°ë¥¼ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŒ (cron)
	- í•„ìš”í•œ ì…ë ¥ ë°ì´í„°ê°€ ê°€ëŠ¥í•  ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆìŒ
- ë²ˆë“¤
	- ì½”ë””ë„¤ì´í„°ë¥¼ ê´€ë¦¬ (ê·¸ë£¹í™”)
		- ì¼ê´„ ì¢…ë£Œ

**Apache Zeppelin**
- Spark ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹ ì†íˆ ì‹¤í—˜í•˜ê³  ë¹…ë°ì´í„°ë¥¼ ì‹œê°í™”í•´ ë¶„ì„í•˜ëŠ”ë° ì‚¬ìš©
- ë°ì´í„°ì— ìŠ¤í¬ë¦½íŠ¸ì™€ ì½”ë“œë¥¼ ìƒí˜¸ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒ
	- ipynb notebook

**Hue**
- ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•˜ê³  ë…¸íŠ¸ë¶ ìƒí˜¸ì‘ìš© ë° hdfs íŒŒì¼ ì‹œìŠ¤í…œì„ íƒìƒ‰í•˜ëŠ” ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
	- Ambari + Zeppelin

**Ganglia**
- í•˜ë‘¡ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- Ambari, Cloudera Manager, Grafana ê°€ ì´ë¥¼ ëŒ€ì²´

**Chukwa**
- hadoop clusterì˜ ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„
- Flume/Kafka ê°€ ì´ë¥¼ ëŒ€ì²´

**Kafka**
- Pub/Sub Message system
- ë°ì´í„°ê°€ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ í´ëŸ¬ìŠ¤í„°ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
	- ì›¹ì„œë²„ì˜ ê³ ê° í–‰ë™ ë°ì´í„° ë¡œê·¸ -> RDBMS
	- internet sensor data
	- stock trading

- **Data source -> Cluster**
	- Kafka handles this
- **What to do with this data?**


**general purpose publish/subscribe messaging system**
 - store all incomming messages from publishers from some period of time and publishes them to a stream of data called **topic**
	- web/sensor -> kafka - publish topic -> subscribers
- consumers basically subscribe to one or more topics and they will receive data as its published
- kafka store the cutoff point so consumers can catch up from where they last left off
	- can pick up whenever they want to


**Flume**

#### Flume ê°œìš”

- **Hadoop ì „ìš© ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬**ë¡œ, Kafkaë³´ë‹¤ Hadoopì— íŠ¹í™”ë˜ì–´ ìˆìŒ.
- ì£¼ë¡œ **ë¡œê·¸ ì§‘ê³„(Log Aggregation)** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œë¨.
- ì—¬ëŸ¬ ì›¹ ì„œë²„ì—ì„œ ë°œìƒí•˜ëŠ” ë¡œê·¸ ë°ì´í„°ë¥¼ ì•ˆì •ì ìœ¼ë¡œ **HDFS, HBase, Hive ë“±**ì— ì „ë‹¬í•˜ëŠ” ì—­í• .
- HDFSê°€ ë™ì‹œì— ë§ì€ ì—°ê²°ì„ ì‹«ì–´í•˜ê³ , ë¡œê·¸ ë°ì´í„°ê°€ **ìŠ¤íŒŒì´í¬(ê¸‰ê²©í•œ ì¦ê°€)** í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, Flumeì€ **ë²„í¼ ì—­í• **ì„ í•˜ë©° ì•ˆì •ì ìœ¼ë¡œ ì „ì†¡ì„ ë³´ì¥.
#### Flume Agent êµ¬ì¡°

Flume AgentëŠ” ì„¸ ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë¨:

1. **Source (ë°ì´í„° ìˆ˜ì§‘ì§€)**
    - ë¡œê·¸ íŒŒì¼, Kafka, Netcat(TCP í¬íŠ¸), HTTP, Exec(ëª…ë ¹ì–´ ì¶œë ¥), Avro/Thrift ë“± ë‹¤ì–‘í•œ ì…ë ¥ ì†ŒìŠ¤ ì§€ì›.
    - **Channel Selector**ë¥¼ í†µí•´ ë°ì´í„°ì˜ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì±„ë„ë¡œ ë¶„ê¸° ê°€ëŠ¥.
    - **Interceptor**ë¥¼ ì´ìš©í•´ ìˆ˜ì§‘ ì‹œ ë°ì´í„° ë³€í˜•(íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ ë“±) ê°€ëŠ¥.
        
2. **Channel (ë°ì´í„° ì´ë™ ê²½ë¡œ)**
    - **Memory Channel**: ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ì¥ì•  ë°œìƒ ì‹œ ë°ì´í„° ìœ ì‹¤ ê°€ëŠ¥.        
    - **File Channel**: ë””ìŠ¤í¬ì— ê¸°ë¡í•´ ì¥ì•  ì‹œì—ë„ ë°ì´í„° ë³´ì¡´ ê°€ëŠ¥.
        
3. **Sink (ë°ì´í„° ì €ì¥ì§€)**
    - HDFS, Hive, HBase, Elasticsearch, Kafka ë“± ë‹¤ì–‘í•œ ëª©ì ì§€ì— ì €ì¥ ê°€ëŠ¥.
    - SinkëŠ” í•˜ë‚˜ì˜ Channelì—ë§Œ ì—°ê²° ê°€ëŠ¥.
    - Kafkaì™€ ë‹¬ë¦¬ Flumeì€ ë°ì´í„°ë¥¼ **ì†Œë¹„ í›„ ì‚­ì œ**(ì„ì‹œ ë²„í¼ ì„±ê²©).
        
#### ì•„í‚¤í…ì²˜ íŒ¨í„´

- **ë©€í‹° í‹°ì–´(fan-in) êµ¬ì¡°** ê°€ëŠ¥:
    - 1ì°¨ Flume AgentëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë²„ ê·¼ì²˜ì—ì„œ ë¡œê·¸ ìˆ˜ì§‘.
    - 2ì°¨ AgentëŠ” ì´ë¥¼ ëª¨ì•„ ìµœì¢…ì ìœ¼ë¡œ HDFSì— ì €ì¥.
    - ì´ëŸ° êµ¬ì¡°ë¥¼ í†µí•´ **ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ë¶„ì‚°**, **ë°ì´í„°ì„¼í„° ê°„ íš¨ìœ¨ì  ì „ì†¡**ì´ ê°€ëŠ¥.
    - Agent ê°„ ì—°ê²°ì—ëŠ” ì£¼ë¡œ **Avro í”„ë¡œí† ì½œ** ì‚¬ìš©.
#### ì‹¤ìŠµ ì˜ˆì œ

1. **Netcat Source + Logger Sink**  
    - TCP í¬íŠ¸(Netcat)ë¡œ ë°ì´í„° ì…ë ¥ â†’ Memory Channel â†’ Logger Sinkì—ì„œ ë¡œê·¸ ì¶œë ¥.
    - ê°„ë‹¨íˆ ë°ì´í„° íë¦„ì„ í™•ì¸í•˜ëŠ” ë° ì‚¬ìš©.
        
2. **Spooldir Source + HDFS Sink**
    - ë””ë ‰í† ë¦¬ì— ë–¨ì–´ì§€ëŠ” ë¡œê·¸ íŒŒì¼ ê°ì‹œ.
    - Interceptor(íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€) ì ìš©.
    - Memory Channel â†’ HDFS Sinkë¡œ ì „ë‹¬.
    - ì €ì¥ ì‹œ **ì—°-ì›”-ì¼-10ë¶„ ë‹¨ìœ„ ë””ë ‰í† ë¦¬ êµ¬ì¡°**ë¡œ ê¸°ë¡, í™•ì¥ì„± ìˆëŠ” ë°ì´í„° ê´€ë¦¬ ê°€ëŠ¥.


**Spark Streaming**

#### ğŸ”¹ ë°°ê²½

- ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬(batch processing)ëŠ” ë°ì´í„°ê°€ ìŒ“ì¼ ë•Œë§ˆë‹¤ í•œ ë²ˆì”© ë¶„ì„ â†’ **ì‹¤ì‹œê°„ì„± ë¶€ì¡±**.
- ì›¹ ë¡œê·¸, IoT ì„¼ì„œ ë°ì´í„° ë“±ì€ 24/7 ê³„ì† ë“¤ì–´ì˜¤ê¸° ë•Œë¬¸ì—, **ì‹¤ì‹œê°„ ë¶„ì„ í•„ìš”ì„±**ì´ ì¦ê°€.
- í•´ê²°ì±…: **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (Spark Streaming, Apache Storm ë“±)**.
#### ğŸ”¹ Spark Streaming ê¸°ë³¸ ê°œë…

- **Spark Streaming**ì€ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ **ë§ˆì´í¬ë¡œ ë°°ì¹˜(micro-batch)** ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ì²˜ë¦¬.
    - ì˜ˆ: 1ì´ˆë§ˆë‹¤ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ë¥¼ í•œ ë©ì–´ë¦¬(RDD)ë¡œ ë¬¶ì–´ ì²˜ë¦¬.
    - ì™„ì „í•œ ì‹¤ì‹œê°„ì€ ì•„ë‹ˆì§€ë§Œ, **ì‚¬ì‹¤ìƒ ì‹¤ì‹œê°„ ì²˜ë¦¬ì— ê°€ê¹Œì›€**.

- Spark í´ëŸ¬ìŠ¤í„°ì˜ **ì—¬ëŸ¬ Receiver**ê°€ ë°ì´í„°ë¥¼ ë°›ê³ , ì´ë¥¼ **RDDë¡œ ë¶„í• (discretize)**.
- ì´ RDDë“¤ì˜ ì§‘í•©ì„ **DStream(Discretized Stream)** ì´ë¼ ë¶€ë¦„.
- DStreamì— ëŒ€í•´ **map, flatMap, filter, reduceByKey** ë“± RDD ì—°ì‚° ê°€ëŠ¥.
- ìƒíƒœ(State) ê´€ë¦¬ ê°€ëŠ¥: ì„¸ì…˜ ë°ì´í„°, ëˆ„ì  í•©ê³„ ë“± ì¥ê¸°ì ì¸ ìƒíƒœ ìœ ì§€.
#### ğŸ”¹ ìœˆë„ìš°(Window) ì—°ì‚°

- ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ì„œ ì¤‘ìš”í•œ ê°œë…ì€ **Windowing**.  
- 3ê°€ì§€ ì£¼ìš” íŒŒë¼ë¯¸í„°:
    1. **Batch Interval**: ë°ì´í„°ë¥¼ ì˜ë¼ë‚´ëŠ” ë‹¨ìœ„ (ì˜ˆ: 1ì´ˆ).
    2. **Window Interval**: ë¶„ì„ ëŒ€ìƒ ê¸°ê°„ (ì˜ˆ: ìµœê·¼ 1ì‹œê°„).
    3. **Slide Interval**: ì–¼ë§ˆë§ˆë‹¤ ê²°ê³¼ë¥¼ ê³„ì‚°í• ì§€ (ì˜ˆ: 30ë¶„ë§ˆë‹¤).
- ì˜ˆ: ìµœê·¼ 1ì‹œê°„ ë™ì•ˆì˜ ì¸ê¸° ìƒí’ˆì„ 30ë¶„ë§ˆë‹¤ ê³„ì‚°.
#### ğŸ”¹ ì½”ë“œ ì˜ˆì‹œ (Python)
```python
ssc = StreamingContext(sc, 1)   
# batch interval = 1ì´ˆ dstream = ... 
# DStream ì •ì˜ 
windowed_counts = dstream.reduceByKeyAndWindow(     lambda x, y: x + y,       # ì¶”ê°€ ë¡œì§     
lambda x, y: x - y,       
# ì œê±° ë¡œì§     
300, 1                    
# window=300ì´ˆ(5ë¶„), slide=1ì´ˆ 
)

```

- ë§¤ 1ì´ˆë§ˆë‹¤, ìµœê·¼ 5ë¶„ê°„ ë°ì´í„°ë¥¼ í•©ì‚°í•´ ê²°ê³¼ ì¶œë ¥.
#### ğŸ”¹ Structured Streaming (ì‹ ë²„ì „)

- **DStream â†’ Dataset(DataFrame ê¸°ë°˜) API**ë¡œ ë°œì „.
- ê°œë…: **ëë‚˜ì§€ ì•ŠëŠ” DataFrame** â†’ ìƒˆë¡œìš´ ë°ì´í„°ê°€ í–‰(Row) í˜•íƒœë¡œ ê³„ì† ì¶”ê°€ë¨.
- ì¥ì :
    - **Batch ì½”ë“œì™€ Streaming ì½”ë“œì˜ ì°¨ì´ ìµœì†Œí™”** â†’ ì½”ë“œ ì¬ì‚¬ìš© ìš©ì´.
    - Spark MLlib ë“± ë‹¤ë¥¸ Dataset ê¸°ë°˜ APIì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ë™.
    - **ë¨¸ì‹ ëŸ¬ë‹ ì‹¤ì‹œê°„ ì ìš© ê°€ëŠ¥**.
        
- ì˜ˆ: S3 ë²„í‚·ì˜ ë¡œê·¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ â†’ ì§‘ê³„ â†’ DB ì €ì¥ (ëª‡ ì¤„ ì½”ë“œë¡œ êµ¬í˜„ ê°€ëŠ¥).
#### ğŸ”¹ Flume + Spark Streaming ì˜ˆì œ

1. **Flume**ì´ ë””ë ‰í† ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§ (spooldir).
2. ë¡œê·¸ë¥¼ **Avro í¬ë§·**ìœ¼ë¡œ í¬íŠ¸ì— ì†¡ì¶œ.
3. **Spark Streaming**ì´ í•´ë‹¹ í¬íŠ¸ë¥¼ ë¦¬ìŠ¤ë‹í•˜ì—¬ ë°ì´í„° ìˆ˜ì‹ .
4. Spark Streamingì´ ë¡œê·¸ ë‚´ URL ë“±ì¥ íšŸìˆ˜ë¥¼ 5ë¶„ ìœˆë„ìš° ë‹¨ìœ„ë¡œ ì§‘ê³„.
5. ê²°ê³¼ë¥¼ ì½˜ì†”(ë˜ëŠ” HDFS, DB)ì— ì¶œë ¥.

ğŸ‘‰ ìš”ì•½í•˜ìë©´, **Spark Streamingì€ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹**ì´ë©°,  
**ìœˆë„ìš° ì—°ì‚°**ê³¼ **ìƒíƒœ ê´€ë¦¬**ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.  
ë˜í•œ **Structured Streaming**ìœ¼ë¡œ ë°œì „í•˜ë©´ì„œ, **Dataset ê¸°ë°˜ í†µí•© API**ë¡œ ì‹¤ì‹œê°„ ë¨¸ì‹ ëŸ¬ë‹ê¹Œì§€ ê°€ëŠ¥í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

#### ğŸ”¹ Apache Storm ê°œìš”

- **ì‹¤ì‹œê°„(event-by-event) ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬**.
- Spark Streamingê³¼ ë¹„ìŠ·í•˜ê²Œ ì—°ì† ë°ì´í„° ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ì§€ë§Œ, Sparkê°€ **ë§ˆì´í¬ë¡œ ë°°ì¹˜(micro-batch)** ê¸°ë°˜ì¸ ë°˜ë©´ Stormì€ **ê°œë³„ ì´ë²¤íŠ¸ ë‹¨ìœ„**ë¡œ ì²˜ë¦¬.
- ë”°ë¼ì„œ **ì„œë¸Œ ì´ˆ ë‹¨ìœ„ ì§€ì—°(sub-second latency)** ê°€ í•„ìš”í•œ ê²½ìš° Stormì´ ìœ ë¦¬.
- YARN ìœ„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œëŠ” **ë…ë¦½ì ì¸ ì‹œìŠ¤í…œ**.
#### ğŸ”¹ í•µì‹¬ ê°œë…

- **Stream**: ì—°ì†ì ìœ¼ë¡œ íë¥´ëŠ” ë°ì´í„°(íŠœí”Œ ë‹¨ìœ„).
- **Tuple**: ë°ì´í„° ë¬¶ìŒ(ì˜ˆ: [5, 7, 33] ê°™ì€ ë¦¬ìŠ¤íŠ¸).
- **Spout**: ë°ì´í„° ì†ŒìŠ¤(ì˜ˆ: Kafka, Twitter API, DB, TCP í¬íŠ¸ ë“±).
- **Bolt**: ë°ì´í„° ì²˜ë¦¬(ë³€í™˜, ì§‘ê³„, ì €ì¥ ë“±).
- **Topology**: Spoutê³¼ Boltë“¤ì„ ì—°ê²°í•œ êµ¬ì¡°(Storm ì• í”Œë¦¬ì¼€ì´ì…˜). Sparkì˜ DAGê³¼ ìœ ì‚¬.
#### ğŸ”¹ ì•„í‚¤í…ì²˜

- **Nimbus**: ì¤‘ì•™ ê´€ë¦¬ ë…¸ë“œ(Job Tracker ì—­í• ). ë‹¨ì¼ ì¥ì• ì (SPOF)ì´ì§€ë§Œ HA êµ¬ì„±ì´ ê°€ëŠ¥.
- **Supervisor**: ì‹¤ì œ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ì›Œì»¤ ë…¸ë“œ ê´€ë¦¬.
- **Zookeeper**: í´ëŸ¬ìŠ¤í„° ì½”ë””ë„¤ì´ì…˜ ë‹´ë‹¹(ê³ ê°€ìš©ì„±).
#### ğŸ”¹ ê°œë°œ ë°©ì‹

- ì£¼ë¡œ **Java**ë¡œ ê°œë°œ (ì´ë¡ ì ìœ¼ë¡œëŠ” ëª¨ë“  ì–¸ì–´ ê°€ëŠ¥í•˜ë‚˜ ì‹¤ì œ ì˜ˆì œëŠ” ê±°ì˜ Java ì¤‘ì‹¬).
- **Storm Core**: ì €ìˆ˜ì¤€ API, _at least once_ ë³´ì¥ (ì¤‘ë³µ ê°€ëŠ¥).
- **Trident**: ê³ ìˆ˜ì¤€ API, _exactly once_ ë³´ì¥.
#### ğŸ”¹ Spark Streaming vs. Storm

| ë¹„êµ í•­ëª© | Spark Streaming                   | Apache Storm                 |
| ----- | --------------------------------- | ---------------------------- |
| ì²˜ë¦¬ ë‹¨ìœ„ | ë§ˆì´í¬ë¡œ ë°°ì¹˜ (1ì´ˆ ë‹¨ìœ„ ë“±)                 | ì´ë²¤íŠ¸ ë‹¨ìœ„ (ì§„ì§œ ì‹¤ì‹œê°„)              |
| ì§€ì—° ì‹œê°„ | ë³´í†µ â‰¥ 1ì´ˆ                           | ì„œë¸Œ ì´ˆ                         |
| ì–¸ì–´    | Scala, Python, Java               | ì£¼ë¡œ Java                      |
| API   | DStream, Structured Streaming     | Core, Trident                |
| ì¥ì     | Spark ìƒíƒœê³„(MLlib, GraphX ë“±)ì™€ í†µí•© ìš©ì´ | ì§„ì§œ ì‹¤ì‹œê°„, íŠœí”Œ ë‹¨ìœ„ ì²˜ë¦¬             |
| ìœˆë„ìš°   | Sliding Window ì¤‘ì‹¬                 | Sliding + Tumbling Window ì§€ì› |
| í™œìš© ì˜ˆ  | ë¡œê·¸ ë¶„ì„, ML ì‹¤ì‹œê°„ ì²˜ë¦¬                  | ì´ˆì €ì§€ì—° ì²˜ë¦¬, Kafkaì™€ ìì£¼ í•¨ê»˜ ì‚¬ìš©     |
#### ğŸ”¹ ìœˆë„ìš° ì²˜ë¦¬

- **Sliding Window**: íŠ¹ì • ì‹œê°„ êµ¬ê°„ì´ ê²¹ì¹˜ë©° ì´ë™ (Sparkì™€ ìœ ì‚¬).
- **Tumbling Window**: ê²¹ì¹˜ì§€ ì•ŠëŠ” ê³ ì • êµ¬ê°„ ë‹¨ìœ„ ì²˜ë¦¬ (ì˜ˆ: ì •í™•íˆ 5ì´ˆë§ˆë‹¤).
#### ğŸ”¹ Stormì˜ í™œìš©

- **Kafka + Storm ì¡°í•©**ì´ ë§ì´ ì‚¬ìš©ë¨ (ì‹¤ì‹œê°„ ë¡œê·¸/ì´ë²¤íŠ¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸).
- Spark Streamingì€ ì ì  ë” ì£¼ëª©ë°›ê³  ìˆìœ¼ë©° MLê³¼ì˜ í†µí•©ì—ì„œ ê°•ì .
- Stormì€ ë¹„êµì  ì„±ìˆ™(stable)í•œ ê¸°ìˆ ì´ë©°, ì‹¤ì‹œê°„ì„±ì´ íŠ¹íˆ ì¤‘ìš”í•œ ê²½ìš° ì„ íƒ.
#### ğŸ”¹ ì˜ˆì œ Topology

- **Spout**: ë¬´ì‘ìœ„ ë¬¸ì¥ ìƒì„±.
- **Bolt 1**: ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬(split).
- **Bolt 2**: ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì¹´ìš´íŠ¸(hash map ê¸°ë°˜).
- **ì¶œë ¥**: ë¡œê·¸ì— ë‹¨ì–´ë³„ ë“±ì¥ íšŸìˆ˜ ì¶œë ¥.
- ì´ í† í´ë¡œì§€ëŠ” **ê³„ì† ì‹¤í–‰ë˜ë©°**, ëª…ì‹œì ìœ¼ë¡œ ì¤‘ë‹¨í•´ì•¼ ë©ˆì¶¤.

#### ğŸ”¹ Apache Flink ê°œìš”

- ì´ë¦„ ìœ ë˜: **Flink = â€œë¹ ë¥´ê³  ë¯¼ì²©í•˜ë‹¤(ë…ì¼ì–´)â€**.
- **ì‹¤ì‹œê°„(event-by-event) ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬** í”„ë ˆì„ì›Œí¬.
- í˜„ì¬ëŠ” ì¶©ë¶„íˆ ì„±ìˆ™í•´ ëŒ€ê·œëª¨ ê¸°ì—…(ì˜ˆ: Capital One, Alibaba)ì—ì„œë„ ì‚¬ìš©.
- Spark Streaming, Stormê³¼ ê²½ìŸí•˜ë©° ë°œì „ ì¤‘.
#### ğŸ”¹ ì£¼ìš” íŠ¹ì§•

1. **ì´ë²¤íŠ¸ ë‹¨ìœ„ ì²˜ë¦¬ (ì§„ì§œ ì‹¤ì‹œê°„)**
    - Spark Streaming(ë§ˆì´í¬ë¡œ ë°°ì¹˜)ì™€ ë‹¬ë¦¬ **ê°œë³„ ì´ë²¤íŠ¸**ë¥¼ ì¦‰ì‹œ ì²˜ë¦¬.
    - Stormê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì„±ëŠ¥(throughput) ë©´ì—ì„œëŠ” Stormë³´ë‹¤ í•œ ë‹¨ê³„ ë¹ ë¦„.
        
2. **í™•ì¥ì„± & ì„±ëŠ¥**
    - ìˆ˜ì²œ ê°œ ë…¸ë“œê¹Œì§€ í™•ì¥ ê°€ëŠ¥.
    - Storm ëŒ€ë¹„ **10ë°° ìˆ˜ì¤€ì˜ ì²˜ë¦¬ëŸ‰** â†’ ë” ì ì€ í•˜ë“œì›¨ì–´ë¡œ ë™ì¼ ì‘ì—… ìˆ˜í–‰ ê°€ëŠ¥.
        
3. **ë‚´ê²°í•¨ì„± (Fault Tolerance)**
    - **State Snapshot** ê¸°ëŠ¥ â†’ ì¥ì•  ë°œìƒ ì‹œì—ë„ ì •í™•íˆ í•œ ë²ˆ(exactly-once) ì²˜ë¦¬ ë³´ì¥.
    - ê¸ˆìœµ ê±°ë˜ ê°™ì€ ë¯¼ê°í•œ ë°ì´í„° ì²˜ë¦¬ì— ì í•©.
        
4. **Event Time ê¸°ë°˜ ì²˜ë¦¬**
    - ë°ì´í„°ê°€ ëŠ¦ê²Œ ë„ì°©í•˜ê±°ë‚˜ ìˆœì„œê°€ ë’¤ë°”ë€Œì–´ë„ **ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ì‹œì  ê¸°ì¤€**ìœ¼ë¡œ ì²˜ë¦¬.
    - ì˜ˆ: ì£¼ì‹ ê±°ë˜ ë°ì´í„°ê°€ ëª‡ ì‹œê°„ ëŠ¦ê²Œ ë„ì°©í•´ë„ ì˜¬ë°”ë¥¸ ì‹œê°„ ìˆœì„œë¡œ ë°˜ì˜.
        
5. **ê°•ë ¥í•œ ìœˆë„ìš° ì—°ì‚°**
    - ë‹¤ì–‘í•œ í˜•íƒœì˜ Window ì§€ì› â†’ ì‹œê°„/ì´ë²¤íŠ¸ ë‹¨ìœ„ë¡œ ìœ ì—°í•œ ì§‘ê³„ ê°€ëŠ¥.
        
#### ğŸ”¹ Spark & Stormê³¼ ë¹„êµ

| ë¹„êµ í•­ëª©  | Spark Streaming                    | Apache Storm        | Apache Flink            |
| ------ | ---------------------------------- | ------------------- | ----------------------- |
| ì²˜ë¦¬ ë°©ì‹  | Micro-batch (1ì´ˆ ë‹¨ìœ„)                | Event-by-Event      | Event-by-Event          |
| ì§€ì—° ì‹œê°„  | ì´ˆ ë‹¨ìœ„                               | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ (sub-second) | ë°€ë¦¬ì´ˆ ë‹¨ìœ„ (ë” ë†’ì€ ì²˜ë¦¬ëŸ‰)       |
| ì¥ì      | Spark ìƒíƒœê³„(MLlib, SQL, GraphX ë“±) í™œìš© | ì§„ì§œ ì‹¤ì‹œê°„              | ì§„ì§œ ì‹¤ì‹œê°„ + ë†’ì€ ì„±ëŠ¥ + ë°°ì¹˜ë„ ì§€ì› |
| ML/SQL | MLlib, Spark SQL                   | ì œí•œì                  | FlinkML, Table API      |
| ì„±ìˆ™ë„    | ì„±ìˆ™ & ê´‘ë²”ìœ„ ì‚¬ìš©                        | ì•ˆì •ì ì´ì§€ë§Œ ì ì°¨ ê°ì†Œì„¸       | ë¹ ë¥´ê²Œ ì„±ì¥ ì¤‘ (ê°€ì¥ ì ŠìŒ)        |

---

#### ğŸ”¹ Flinkì˜ ì´ì¤‘ ì„±ê²©

- **Streaming API**: ë¬´í•œ ë°ì´í„° ìŠ¤íŠ¸ë¦¼(event ë‹¨ìœ„ ì²˜ë¦¬).
- **Batch API**: ìœ í•œ ë°ì´í„°ì…‹(batch) ì²˜ë¦¬.
- ë”°ë¼ì„œ FlinkëŠ” **ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë°°ì¹˜ë¥¼ ëª¨ë‘ ì§€ì›**í•˜ëŠ” ë²”ìš© ì—”ì§„.
#### ğŸ”¹ Flink ìƒíƒœê³„

- Sparkì²˜ëŸ¼ ìì²´ ìƒíƒœê³„ë¥¼ êµ¬ì¶• ì¤‘:
    - **FlinkML** â†’ Spark MLlib ëŒ€ì‘.
    - **Gelly** â†’ Spark GraphX ëŒ€ì‘.
    - **Table API** â†’ Spark SQL ëŒ€ì‘.
        
- ì—°ê²° ê°€ëŠ¥ ì‹œìŠ¤í…œ: **Kafka, HDFS, Cassandra, Elasticsearch, NiFi, Redis, RabbitMQ** ë“±.

**Impala**
- cloudera ë²„ì „ì˜ hive
- ì¿¼ë¦¬ë¥¼ ì‹¤í–‰ ì‹œí‚¤ê¸° ìœ„í•´ í•­ìƒ ëŒ€ê¸°ì¤‘

**Accumulo**
- nosql database that can specify who can see which data

**redis**
- next-generation of memcached
- ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë¹ˆë²ˆíˆ ì•¡ì„¸ìŠ¤ í•˜ëŠ” ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ ì•ˆì— ì €ì¥í•  ìˆ˜ ìˆê²Œ í•¨
	- memcache : key-value pair
	- redis : entire data structure

**ignite**
- redisì˜ ëŒ€ì•ˆ
- ë©”ëª¨ë¦¬ ë‚´ ë°ì´í„° ì €ì¥ì†Œì™€ ì•¡ì„¸ìŠ¤ë¥¼ ì œê³µí•˜ì§€ë§Œ DBì— ê°€ê¹Œì›€
- supports
	- ë©”ëª¨ë¦¬ ë‚´ ë°ì´í„° ì•¡ì„¸ìŠ¤
	- ACID transaction
	- support SQL
- ë¶„ì‚° ë¬¸ì„œ ê²€ìƒ‰ ë° ë¶„ì„ ì—”ì§„

**kinesis**
- aws ë²„ì „ì˜ kafka

**AWS**
- kinesis = kafka
- s3 = hdfs
- cloudsearch = elasticsearch
- dynamodb = cassandra
- rds = mysql
- elasticache = redis
- elastic mapreduce = hadoop cluster

**Apache NiFi**
- ë°ì´í„° ë£¨íŒ…ì˜ ë°©í–¥ì„± ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë°©ë²•
- kafka, hdfs, hive ë“±ê³¼ ì—°ê²° ê°€ëŠ¥
- í´ëŸ¬ìŠ¤í„°ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ì˜ íë¦„ì„ ì²´ê³„í™”í•˜ëŠ” ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©

**Falcon**
- Oozie ìœ„ì—ì„œ ì‘ë™í•˜ëŠ” ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì—”ì§„
- í´ëŸ¬ìŠ¤í„°ì— ë“¤ì–´ì˜¨ ë‹¤ìŒì— ë°ì´í„°ì˜ íë¦„ì„ ê´€ë¦¬í•¨
- oozieì— ë„ˆë¬´ ë§ì€ ì›Œí¬í”Œë¡œê°€ ìˆì–´ì„œ ì¼ì¢…ì˜ ê´€ë¦¬ìê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©

**Apache Slider**
- yarn í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚°í•˜ë ¤ëŠ” ë²”ìš© ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ë„êµ¬
- ì• í”Œë¦¬ì¼€ì´ì…˜ì„ í´ëŸ¬ìŠ¤í„°ì— ë°°ì¹˜í•˜ê³  ëª¨ë‹ˆí„°í•˜ë©° ì‹œê°„ì— ë”°ë¼ ê·¸ ìš©ëŸ‰ì„ ëŠ˜ì´ê±°ë‚˜ ì¤„ì„

**Hadoop = hdfs + yarn + Map Reduce**

**í•­ìƒ ìµœì¢… ì‚¬ìš©ìì˜ ê´€ì ì—ì„œ ë¬¸ì œì— ì ‘ê·¼**
- ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ ê³ ê°ì˜ í•„ìš”ë¥¼ ì¶©ì¡±

1. ìµœì¢… ì‚¬ìš©ìì—ê²Œ ì–´ë–¤ ì•¡ì„¸ìŠ¤ íŒ¨í„´ì„ ì˜ˆìƒí•˜ëŠ”ê°€
	1. í° ë‚ ì§œ ë²”ìœ„ì— ê±¸ì¹œ ë¶„ì„ ì¿¼ë¦¬ì¸ì§€
	2. ëŒ€í˜• ì›¹ì‚¬ì´íŠ¸ì—ì„œ ëŒ€ëŸ‰ì˜ ì‘ì€ íŠ¸ëœì­ì…˜ì´ ìˆê³  ì‹ ì†í•˜ê²Œ ê³„ì‚°í•´ì•¼ í•˜ëŠ” êµ¬ì²´ì ì¸ ì¿¼ë¦¬ì¸ì§€
2. ê°€ìš©ì„±
	1. availability
3. ì¼ê´€ì„± 
	1. consistency
4. Data size
	1. how big a data?
	2. how fast does it grow
		1. cluster and distributed computing
5. Data retention
	1. do you need them for auditing?
	2. how long do you need to keep them alive
	3. privacy policy
6. latency
	1. how fast should 

